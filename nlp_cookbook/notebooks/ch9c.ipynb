{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8acde507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      "  Number of examples: 151\n",
      "  Features: {'id': Value('string'), 'content': Value('string'), 'content_type': Value('string'), 'meta': {'url': Value('string'), '_split_id': Value('int64')}, 'id_hash_keys': List(Value('string')), 'score': Value('null'), 'embedding': Value('null')}\n",
      "\n",
      "First example:\n",
      "{'id': 'b3de1a673c1eb2876585405395a10c3d', 'content': 'The Colossus of Rhodes (Ancient Greek: ὁ Κολοσσὸς Ῥόδιος, romanized:\\xa0ho Kolossòs Rhódios Greek: Κολοσσός της Ρόδου, romanized:\\xa0Kolossós tes Rhódou)[a] was a statue of the Greek sun-god Helios, erected in the city of Rhodes, on the Greek island of the same name, by Chares of Lindos in 280\\xa0BC. One of the Seven Wonders of the Ancient World, it was constructed to celebrate the successful defence of Rhodes city against an attack by Demetrius Poliorcetes, who had besieged it for a year with a large army and navy.\\nAccording to most contemporary descriptions, the Colossus stood approximately 70 cubits, or 33 metres (108 feet) high – approximately the height of the modern Statue of Liberty from feet to crown – making it the tallest statue in the ancient world.[2] It collapsed during the earthquake of 226 BC, although parts of it were preserved. In accordance with a certain oracle, the Rhodians did not build it again.[3] John Malalas wrote that Hadrian in his reign re-erected the Colossus,[4] but he was mistaken.[5] According to the Suda, the Rhodians were called Colossaeans (Κολοσσαεῖς), because they erected the statue on the island.', 'content_type': 'text', 'meta': {'url': 'https://en.wikipedia.org/wiki/Colossus_of_Rhodes', '_split_id': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}\n"
     ]
    }
   ],
   "source": [
    "# Abstractive search example\n",
    "# Download documents\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "\n",
    "# Inspect structure\n",
    "print(\"Dataset info:\")\n",
    "print(f\"  Number of examples: {len(dataset)}\")\n",
    "print(f\"  Features: {dataset.features}\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f71892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 151 documents indexed\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from haystack import Document\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "\n",
    "# Setup document store\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# Convert dataset to Haystack Documents\n",
    "documents = []\n",
    "for item in dataset:\n",
    "    doc = Document(\n",
    "        content=item[\"content\"],  # Adjust field name based on dataset\n",
    "        meta=item  # Store all fields as metadata\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "# Index documents\n",
    "document_store.write_documents(documents)\n",
    "\n",
    "print(f\"✓ {document_store.count_documents()} documents indexed\")\n",
    "\n",
    "# Create retriever\n",
    "retriever = InMemoryBM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b7f4824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a37a465ffac4483956912ea23e5f06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4df0b284b24f76a698050c6cb4bfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cfb361f1644d2f92c4b736376747ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e2f87495f2433493b12c2302689130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c48166fff1e43daabcb64ae45d44b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66ea993bfb04de48f1eca7d2a2e43a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca643208fc2243f7947b7901784ddf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1602 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the Seven Wonders of the Ancient World?\n",
      "Answer: The Great Pyramid of Giza, the Lighthouse of Alexandria, the Statue of Zeus at Olympia, the Colossus of Rhodes, the Mausoleum at Halicarnassus, and the Mausoleum at\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import HuggingFaceLocalGenerator\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "# Assuming you have document_store already set up\n",
    "\n",
    "# 1. Prompt template (Jinja2 syntax)\n",
    "prompt_template = \"\"\"\n",
    "Synthesize a comprehensive answer from the following text for the given question.\n",
    "Provide a clear and concise response that summarizes the key points and information.\n",
    "Your answer should be in your own words and be no longer than 50 words.\n",
    "\n",
    "Related text:\n",
    "{% for document in documents %}\n",
    "{{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# 2. Create components\n",
    "retriever = InMemoryBM25Retriever(document_store=document_store)\n",
    "prompt_builder = PromptBuilder(template=prompt_template)\n",
    "generator = HuggingFaceLocalGenerator(\n",
    "    model=\"google/flan-t5-large\",\n",
    "    task=\"text2text-generation\",\n",
    "    generation_kwargs={\"max_new_tokens\": 50}\n",
    ")\n",
    "\n",
    "# 3. Build RAG pipeline\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"generator\", generator)\n",
    "\n",
    "# 4. Connect components\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"generator\")\n",
    "\n",
    "# 5. Run query\n",
    "question = \"What are the Seven Wonders of the Ancient World?\"\n",
    "\n",
    "result = rag_pipeline.run({\n",
    "    \"retriever\": {\"query\": question, \"top_k\": 5},\n",
    "    \"prompt_builder\": {\"query\": question},\n",
    "    \"generator\": {}\n",
    "})\n",
    "\n",
    "# 6. Print answer\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {result['generator']['replies'][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06d2b0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the Great Pyramid of Giza\n",
      "Answer: The Great Pyramid of Giza is the largest Egyptian pyramid and the tomb of Fourth Dynasty pharaoh Khufu.\n"
     ]
    }
   ],
   "source": [
    "# 5. Run query\n",
    "question = \"What is the Great Pyramid of Giza\"\n",
    "\n",
    "result = rag_pipeline.run({\n",
    "    \"retriever\": {\"query\": question, \"top_k\": 5},\n",
    "    \"prompt_builder\": {\"query\": question},\n",
    "    \"generator\": {}\n",
    "})\n",
    "\n",
    "# 6. Print answer\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {result['generator']['replies'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "121412d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bd4b1c39194c9a998ca9ab48d4885d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30114288031a484f9e864654f280eae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04accab6adb240d3af6f1ddf8cd0b258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09ed79269e04a1693e2148ad5e67b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9fc66b3c9d4f1dbf24db61a9f69090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Text summarization with T5\n",
    "from transformers import pipeline\n",
    "summarization_pipe = pipeline(\"summarization\", model=\"t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898625b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Sherlock Holmes:\n",
      "in his eyes she eclipses and predominates the whole of her sex . as a lover he would have placed himself in a false position . he never spoke of the softer passions, save with a gibe and a sneer .\n"
     ]
    }
   ],
   "source": [
    "# Summarize a short Sherlock Holmes text\n",
    "with open(\"../data/sherlock_holmes_1.txt\", \"r\") as file:\n",
    "    sherlock = file.read()\n",
    "summary = summarization_pipe(sherlock, max_length=150, min_length=40, do_sample=False)\n",
    "print(\"Summary of Sherlock Holmes:\")\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e958ff8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bcdad011854a1a87d63d38cf615d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbb2dba9d3a46559cbe6727776f361e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdef97cb02a450bb979123bd9b661dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30de8d710514616abb3f11fb0787b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7416e08e4a604152be44e7b96c8c2eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a444c74c1c44495382f824ca19228e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Sherlock Holmes (BART):\n",
      "Sherlock Holmes never felt any emotionakin to love for Irene Adler. All emotions, and that one particularly, were abhorrent to his cold, precise mind. To admit such intrusions into his own delicate and finelyadjusted temperament was to introduce a distracting factor.\n"
     ]
    }
   ],
   "source": [
    "# Switch to BART\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "summarization_pipe_bart = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if device==\"cuda\" else -1)\n",
    "summary_bart = summarization_pipe_bart(sherlock, max_length=150, min_length=40, do_sample=False)\n",
    "print(\"Summary of Sherlock Holmes (BART):\")\n",
    "print(summary_bart[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e54acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textual entailment with T5\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d793256a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d17645d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment result: entailment\n"
     ]
    }
   ],
   "source": [
    "premise = \"Large language models are capable of performing a variety of natural language processing tasks.\"\n",
    "hypothesis = \"LLMs can do many NLP jobs.\"\n",
    "input_ids = tokenizer.encode(f\"mnli: {premise} </s> {hypothesis}\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(input_ids)\n",
    "entailment = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Entailment result: {entailment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc7caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment: entailment\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\").to(device)\n",
    "\n",
    "# Input\n",
    "premise = \"Large language models are capable of performing a variety of natural language processing tasks.\"\n",
    "hypothesis = \"LLMs can do many NLP jobs.\"\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(\n",
    "        f\"mnli: {premise} </s> {hypothesis}\", # T5 specific input format\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=10,\n",
    "        num_beams=1\n",
    "    )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Entailment: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5744a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: LLMs can do many NLP jobs.\n",
      "Entailment: entailment\n",
      "\n",
      "Hypothesis: Neural networks cannot process text.\n",
      "Entailment: neutral\n",
      "\n",
      "Hypothesis: Language models are useful for translation.\n",
      "Entailment: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\").to(device)\n",
    "\n",
    "# Input: one premise, multiple hypotheses\n",
    "premise = \"Large language models are capable of performing a variety of natural language processing tasks.\"\n",
    "hypotheses = [\n",
    "    \"LLMs can do many NLP jobs.\",\n",
    "    \"Neural networks cannot process text.\",\n",
    "    \"Language models are useful for translation.\"\n",
    "]\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    # Create batch of premise-hypothesis pairs\n",
    "    texts = [f\"mnli: {premise} </s> {hyp}\" for hyp in hypotheses]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=10,\n",
    "        num_beams=1\n",
    "    )\n",
    "    \n",
    "    results = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "# Display results\n",
    "for hyp, result in zip(hypotheses, results):\n",
    "    print(f\"Hypothesis: {hyp}\")\n",
    "    print(f\"Entailment: {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75d468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef860942f9f84c18b72b892ba1fcc4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8826d610e00c42c59b6edf179bf8b2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f611f3563334f61a44e615b3359e4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f3cd2a66e34790becee7b6b9e57c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdc22f315ec4809814c93b4b9bd98d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e691711840274e168eee08b88bb06eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc5b0e177224413b2c69495d13e018d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: The movie was fantastic! I really loved it.\n",
      "Prediction: {'label': 'POSITIVE', 'score': 0.9989079236984253}\n",
      "\n",
      "Top contributing words for POSITIVE sentiment:\n",
      "  it                   +0.0124\n",
      "  fantastic            +0.0124\n",
      "  was                  +0.0117\n",
      "  loved                +0.0113\n",
      "  I                    +0.0092\n",
      "  movie                +0.0056\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34faec9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
