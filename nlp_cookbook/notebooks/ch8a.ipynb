{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64705e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, get_dataset_split_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24b8f4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation', 'test']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "print(get_dataset_split_names(\"rotten_tomatoes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "595873fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 8530\n",
      "\n",
      "{'text': Value('string'), 'label': ClassLabel(names=['neg', 'pos'])}\n"
     ]
    }
   ],
   "source": [
    "training_data = dataset[\"train\"]\n",
    "print(f\"Number of training examples: {len(training_data)}\")\n",
    "print(dataset['train'].info.description)\n",
    "print(training_data.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcacfebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "\n",
      "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "\n",
      "effective but too-tepid biopic\n",
      "\n",
      "if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\n",
      "\n",
      "emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = training_data[\"text\"][:5]\n",
    "for sentence in sentences:\n",
    "    print(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3f78acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e20a62258b2401c9296b6d0e6a0d40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ee361e78ff4defa76dbf9a7c512b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a840859acda84c52a5cc95ce589e346f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4901a64c164e3c8fb4acf7984828f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1103, 2067, 1110, 17348, 1106, 1129, 1103, 6880, 1432, 112, 188, 1207, 107, 14255, 1389, 107, 1105, 1115, 1119, 112, 188, 1280, 1106, 1294, 170, 24194, 1256, 3407, 1190, 170, 11791, 5253, 188, 1732, 7200, 10947, 12606, 2895, 117, 179, 7766, 118, 172, 15554, 1181, 3498, 6961, 3263, 1137, 188, 1566, 7912, 14516, 6997, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "sentence = training_data[\"text\"][0]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokens_input = tokenizer(sentence)\n",
    "print(tokens_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "450641ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'\", 's', 'new', '\"', 'con', '##an', '\"', 'and', 'that', 'he', \"'\", 's', 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'a', '##rno', '##ld', 's', '##ch', '##war', '##zen', '##eg', '##ger', ',', 'j', '##ean', '-', 'c', '##lau', '##d', 'van', 'dam', '##me', 'or', 's', '##te', '##ven', 'se', '##gal', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokens_input['input_ids'])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "221da98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seniment analysis with Pretrained Transformers\n",
    "from datasets import load_dataset\n",
    "from evaluate import evaluator, combine\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a3c02ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n",
      "consistently clever and suspenseful .\n",
      "it's like a \" big chill \" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\n",
      "the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\n",
      "red dragon \" never cuts corners .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1\n",
    "sentences = load_dataset(\"rotten_tomatoes\", split=\"test\").select(range(5))\n",
    "[print(sentence) for sentence in sentences[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76745346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-rotten-tomatoes were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline\n",
    "roberta_pipeline = pipeline(\"sentiment-analysis\",\n",
    "                            model=\"textattack/roberta-base-rotten-tomatoes\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e7bbe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9650675058364868}, {'label': 'LABEL_1', 'score': 0.9958857893943787}, {'label': 'LABEL_0', 'score': 0.9162591099739075}, {'label': 'LABEL_1', 'score': 0.9955574870109558}, {'label': 'LABEL_1', 'score': 0.8789991736412048}]\n"
     ]
    }
   ],
   "source": [
    "# classification\n",
    "text_list = list(sentences[\"text\"])\n",
    "predictions = roberta_pipeline(text_list)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79fa9448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n",
      "Actual: 1\n",
      "Prediction: LABEL_1\n",
      "Score: 0.9650675058364868\n",
      "\n",
      "Sentence: consistently clever and suspenseful .\n",
      "Actual: 1\n",
      "Prediction: LABEL_1\n",
      "Score: 0.9958857893943787\n",
      "\n",
      "Sentence: it's like a \" big chill \" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\n",
      "Actual: 1\n",
      "Prediction: LABEL_0\n",
      "Score: 0.9162591099739075\n",
      "\n",
      "Sentence: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\n",
      "Actual: 1\n",
      "Prediction: LABEL_1\n",
      "Score: 0.9955574870109558\n",
      "\n",
      "Sentence: red dragon \" never cuts corners .\n",
      "Actual: 1\n",
      "Prediction: LABEL_1\n",
      "Score: 0.8789991736412048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(text_list):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Actual: {sentences['label'][idx]}\")\n",
    "    print(f\"Prediction: {predictions[idx]['label']}\")\n",
    "    print(f\"Score: {predictions[idx]['score']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "683e9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate inference for the entire test set\n",
    "test_dataset = load_dataset(\"rotten_tomatoes\", split=\"test\")\n",
    "task_evaluator = evaluator(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4c69c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3306cb9926e4467cbc6c77f6a4bd6043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f27f9d530d42309619cd94cb40c5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a0473cdbfb4439a99c15c5f5cd9a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6fa9d10dd143fe8af01a67871b36a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=roberta_pipeline,\n",
    "    data=test_dataset,\n",
    "    metric=combine([\"accuracy\", \"precision\", \"recall\", \"f1\"]),\n",
    "    label_mapping={\"LABEL_0\": 0, \"LABEL_1\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43dd0848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "accuracy...................... 0.8874\n",
      "precision..................... 0.9223\n",
      "recall........................ 0.8462\n",
      "f1............................ 0.8826\n",
      "total_time_in_seconds......... 88.4583\n",
      "samples_per_second............ 12.0509\n",
      "latency_in_seconds............ 0.0830\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key:.<30} {value:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82f8725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot classification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f93ad56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec6314678814502bdabbc39222d1525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bf5711e4874249a241a53be720944e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb1ab9f649c4aa4b3031f8849f241a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc2404dbb9f42588330d70f677f6e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674b5e4dc0c445349da4fdec3adbd113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca435a2634a040a7aa5a9b8230ad3a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "zero_shot_pipe = pipeline(\n",
    "    model=\"facebook/bart-large-mnli\", \n",
    "    task=\"zero-shot-classification\", \n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bd5f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = zero_shot_pipe(\n",
    "    \"I am so hooked to NLP modeling that I work late every night.\",\n",
    "    candidate_labels=[\"technology\", \"gaming\", \"hobby\", \"art\", \"computer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6e6659a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am so hooked to NLP modeling that I work late every night.\n",
      "technology..... 0.6315\n",
      "hobby.......... 0.2763\n",
      "computer....... 0.0664\n",
      "art............ 0.0166\n",
      "gaming......... 0.0091\n"
     ]
    }
   ],
   "source": [
    "print(result[\"sequence\"])\n",
    "for i, label in enumerate(result[\"labels\"]):\n",
    "    print(f\"{label:.<15} {result['scores'][i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "202e373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting text\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4004c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I think the next big breakthrough in AI will be\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d4f6dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4899032b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "generated_outputs = generator(\n",
    "    text,\n",
    "    max_length=50,\n",
    "    num_return_sequences=5,\n",
    "    num_beams=5,\n",
    "    pad_token_id=50256  # to avoid warnings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78257da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence.\n",
      "I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the\n",
      "I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will\n",
      "I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in AI. I\n",
      "I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in Artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "for output in generated_outputs:\n",
    "    print(output['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f812cc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence.\n",
      "I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the\n",
      "I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will be in AI. I think the next big breakthrough in AI will\n",
      "I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in AI. I\n",
      "I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in artificial intelligence. I think the next big breakthrough in AI will be in Artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "generated_outputs2 = generator(\n",
    "    text,\n",
    "    max_length=50,\n",
    "    num_return_sequences=5,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    pad_token_id=50256  # to avoid warnings\n",
    ")\n",
    "for output in generated_outputs:\n",
    "    print(output['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ecc505e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think the next big breakthrough in AI will be in machine learning,\" he said.\n",
      "\n",
      "\"It's going to be very interesting to see how it evolves over time.\"\n"
     ]
    }
   ],
   "source": [
    "generated_outputs2 = generator(\n",
    "    text,\n",
    "    max_length=500,\n",
    "    num_return_sequences=1,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    top_k=50,\n",
    "    top_p=0.85,\n",
    "    pad_token_id=50256  # to avoid warnings\n",
    ")\n",
    "for output in generated_outputs2:\n",
    "    print(output['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "caf8e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language translation\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af94be1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenizer = \u001b[43mT5Tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mt5-base\u001b[39m\u001b[33m\"\u001b[39m, model_max_length=\u001b[32m512\u001b[39m)\n\u001b[32m      3\u001b[39m model = T5ForConditionalGeneration.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mt5-base\u001b[39m\u001b[33m\"\u001b[39m, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m model = model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/learning/learning_python/.venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:2157\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   2155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mis_dummy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mmro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m2157\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/learning/learning_python/.venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:2143\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   2140\u001b[39m         failed.append(msg.format(name))\n\u001b[32m   2142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m2143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "    \"t5-base\", model_max_length=512)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53d2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
