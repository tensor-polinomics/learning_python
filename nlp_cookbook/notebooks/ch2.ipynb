{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d2db182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.14 (main, Oct 31 2025, 23:04:14) [Clang 21.1.4 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "504b8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"../util/file_utils.ipynb\"\n",
    "%run -i \"../util/lang_utils.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc862b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I have five birds\"\n",
    "doc = small_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba1f9985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birds: plural\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if (token.pos_ == \"NOUN\" and\n",
    "        token.lemma_ != token.text):\n",
    "        print(f\"{token.text}: plural\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "412dedc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Plur']\n"
     ]
    }
   ],
   "source": [
    "print(doc[3].morph.get(\"Number\"))  # Output: ['Plur']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00e89b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Enum class to define custom labels\n",
    "from enum import Enum\n",
    "class Noun_number(Enum):\n",
    "    SINGULAR = 1\n",
    "    PLURAL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de361cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine singular or plural nouns\n",
    "def get_nouns_number(text, model, method=\"lemma\"):\n",
    "    nouns = []\n",
    "    doc = model(text)\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"NOUN\"):\n",
    "            if method == \"lemma\":\n",
    "                if token.lemma_ != token.text:\n",
    "                    nouns.append((token.text, Noun_number.PLURAL))\n",
    "                else:\n",
    "                    nouns.append((token.text, Noun_number.SINGULAR))\n",
    "            elif method == \"morph\":\n",
    "                number = token.morph.get(\"Number\")\n",
    "                if \"Plur\" in number:\n",
    "                    nouns.append((token.text, Noun_number.PLURAL))\n",
    "                else:\n",
    "                    nouns.append((token.text, Noun_number.SINGULAR))\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "964fa4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('geese', <Noun_number.PLURAL: 2>), ('road', <Noun_number.SINGULAR: 1>), ('deer', <Noun_number.SINGULAR: 1>)]\n",
      "[('geese', <Noun_number.PLURAL: 2>), ('road', <Noun_number.SINGULAR: 1>), ('deer', <Noun_number.SINGULAR: 1>)]\n"
     ]
    }
   ],
   "source": [
    "text = \"Three geese crossed the road, but the two deer stayed.\"\n",
    "nouns = get_nouns_number(text, small_model, method=\"lemma\")\n",
    "print(nouns)\n",
    "nouns = get_nouns_number(text, small_model, method=\"morph\")\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dbc2107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('geese', <Noun_number.PLURAL: 2>), ('road', <Noun_number.SINGULAR: 1>), ('deer', <Noun_number.SINGULAR: 1>)]\n",
      "[('geese', <Noun_number.PLURAL: 2>), ('road', <Noun_number.SINGULAR: 1>), ('deer', <Noun_number.SINGULAR: 1>)]\n"
     ]
    }
   ],
   "source": [
    "text = \"Three geese crossed the road, but the two deer stayed.\"\n",
    "nouns = get_nouns_number(text, large_model, method=\"lemma\")\n",
    "print(nouns)\n",
    "nouns = get_nouns_number(text, large_model, method=\"morph\")\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a94be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('geese', 'plural'), ('road', 'singular'), ('deer', 'plural')]\n"
     ]
    }
   ],
   "source": [
    "# GPT 5\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "prompt = \"\"\"Decide whether the noun in the sentence is singular or plural.\n",
    "Return the list in the format of a Python tuple: (word, number), where number is either 'singular' or 'plural'.\n",
    "Do not provide any explanation.\n",
    "Sentence: \"Three geese crossed the road, but the two deer stayed.\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a NLP expert.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a69da90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['books', 'geese', 'pens', 'points', 'deer', 'children']\n",
      "['book', 'goose', 'pen', 'point', 'deer', 'child']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "texts = [\"book\", \"goose\", \"pen\", \"point\", \"deer\", \"child\"]\n",
    "blob_objs = [TextBlob(text) for text in texts]\n",
    "plurals = [blob.words[0].pluralize() for blob in blob_objs]\n",
    "print(plurals)\n",
    "\n",
    "blob_objs = [TextBlob(text) for text in plurals]\n",
    "singulars = [blob.words[0].singularize() for blob in blob_objs]\n",
    "print(singulars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dd27ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency parsing\n",
    "sentence = \"I have seldom heard him mention her under any other name.\"\n",
    "\n",
    "def print_dependencies(sentence, model):\n",
    "    doc = model(sentence)\n",
    "    for token in doc:\n",
    "        print(f\"{token.text:10} {token.dep_:10} {spacy.explain(token.dep_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c13b7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I          nsubj      nominal subject\n",
      "have       aux        auxiliary\n",
      "seldom     advmod     adverbial modifier\n",
      "heard      ROOT       root\n",
      "him        nsubj      nominal subject\n",
      "mention    ccomp      clausal complement\n",
      "her        dobj       direct object\n",
      "under      prep       prepositional modifier\n",
      "any        det        determiner\n",
      "other      amod       adjectival modifier\n",
      "name       pobj       object of preposition\n",
      ".          punct      punctuation\n"
     ]
    }
   ],
   "source": [
    "print_dependencies(sentence, small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88eb384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sherlock Holmes\n",
      "she\n",
      "the_ woman\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "def print_noun_chunks(text, model):\n",
    "    doc = model(text)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        print(chunk.text)\n",
    "        \n",
    "sherlock1 = read_text_file(\"../data/sherlock_holmes_1.txt\")\n",
    "print_noun_chunks(sherlock1[:50], small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e2a2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_properties(sentence, model):\n",
    "    doc = model(sentence)\n",
    "    other_span = \"emotions\"\n",
    "    other_doc = model(other_span)\n",
    "    for noun_chunk in doc.noun_chunks:\n",
    "        print(f\"Noun chunk: {noun_chunk.text}\")\n",
    "        print(f\"Noun chuck start and end: {noun_chunk.start}, {noun_chunk.end}\")\n",
    "        print(f\"Noun chunk sentence: {noun_chunk.sent}\")\n",
    "        print(f\"Noun chunk root text: {noun_chunk.root.text}\")\n",
    "        print(f\"Similarity to '{other_span}': {noun_chunk.similarity(other_doc)}\")\n",
    "    print(f\"\\nSentence similarity to '{other_span}': {doc.similarity(other_doc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5f7d368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun chunk: The study\n",
      "Noun chuck start and end: 0, 2\n",
      "Noun chunk sentence: The study of human emotions is a fascinating field.\n",
      "Noun chunk root text: study\n",
      "Similarity to 'emotions': 0.07203289866447449\n",
      "Noun chunk: human emotions\n",
      "Noun chuck start and end: 3, 5\n",
      "Noun chunk sentence: The study of human emotions is a fascinating field.\n",
      "Noun chunk root text: emotions\n",
      "Similarity to 'emotions': 0.28141409158706665\n",
      "Noun chunk: a fascinating field\n",
      "Noun chuck start and end: 6, 9\n",
      "Noun chunk sentence: The study of human emotions is a fascinating field.\n",
      "Noun chunk root text: field\n",
      "Similarity to 'emotions': 0.10149102658033371\n",
      "\n",
      "Sentence similarity to 'emotions': 0.23275980353355408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2093044/3970430285.py:10: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(f\"Similarity to '{other_span}': {noun_chunk.similarity(other_doc)}\")\n",
      "/tmp/ipykernel_2093044/3970430285.py:11: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(f\"\\nSentence similarity to '{other_span}': {doc.similarity(other_doc)}\")\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The study of human emotions is a fascinating field.\"\n",
    "explore_properties(sentence, small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a09ac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun chunk: The study\n",
      "Noun chuck start and end: 0, 2\n",
      "Noun chunk sentence: The study of human psychology is a fascinating field.\n",
      "Noun chunk root text: study\n",
      "Similarity to 'emotions': 0.29545721411705017\n",
      "Noun chunk: human psychology\n",
      "Noun chuck start and end: 3, 5\n",
      "Noun chunk sentence: The study of human psychology is a fascinating field.\n",
      "Noun chunk root text: psychology\n",
      "Similarity to 'emotions': 0.5463806390762329\n",
      "Noun chunk: a fascinating field\n",
      "Noun chuck start and end: 6, 9\n",
      "Noun chunk sentence: The study of human psychology is a fascinating field.\n",
      "Noun chunk root text: field\n",
      "Similarity to 'emotions': 0.34547051787376404\n",
      "\n",
      "Sentence similarity to 'emotions': 0.4687766432762146\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The study of human psychology is a fascinating field.\"\n",
    "explore_properties(sentence, large_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa756b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject phrase: Laura\n",
      "Object phrase: a very interesting book by the seashore\n"
     ]
    }
   ],
   "source": [
    "def get_subject_phrase(doc):\n",
    "    for token in doc:\n",
    "        if (\"subj\" in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            return doc[start:end]\n",
    "        \n",
    "def get_object_phrase(doc):\n",
    "    for token in doc:\n",
    "        if (\"dobj\" in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            return doc[start:end]\n",
    "        \n",
    "sentence = \"Laura gives Sam a very interesting book by the seashore.\"\n",
    "doc = small_model(sentence)\n",
    "subject_phrase = get_subject_phrase(doc)\n",
    "object_phrase = get_object_phrase(doc)\n",
    "print(f\"Subject phrase: {subject_phrase}\")\n",
    "print(f\"Object phrase: {object_phrase}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29f852",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1235593585.py, line 9)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef_get_prepositonal_phrase(doc):\u001b[39m\n                                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_dative_phrase(doc):\n",
    "    for token in doc:\n",
    "        if (\"dative\" in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            return doc[start:end]\n",
    "\n",
    "def get_prepositonal_phrase(doc):\n",
    "    prep_span = []\n",
    "    for token in doc:\n",
    "        if (\"pobj\" in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            prep_span.append(doc[start:end])\n",
    "    return prep_span\n",
    "\n",
    "sentence = \"Laura gives Sam a very interesting book by the seashore.\"\n",
    "doc = small_model(sentence)\n",
    "dative_phrase = get_dative_phrase(doc)\n",
    "prepositional_phrases = get_prepositional_phrase(doc)\n",
    "print(f\"Dative phrase: {dative_phrase}\")\n",
    "print(f\"Prepositional phrases: {prepositional_phrases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35da00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
