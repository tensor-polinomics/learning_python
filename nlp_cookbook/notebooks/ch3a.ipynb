{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61714976",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"../util/file_utils.ipynb\"\n",
    "%run -i \"../util/lang_utils.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f4e68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560\n",
      "320\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"rotten_tomatoes\", split=\"train[:15%]+train[-15%:]\")\n",
    "test_dataset = load_dataset(\"rotten_tomatoes\", split=\"test[:15%]+test[-15%:]\")\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbbe018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b281cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple text vectorizer\n",
    "class POS_vectorizer:\n",
    "    def __init__(self, spacy_model):\n",
    "        self.model = spacy_model\n",
    "        \n",
    "    def vectorize(self, input_text):\n",
    "        doc = self.model(input_text)\n",
    "        vector = []\n",
    "        vector.append(len(doc))\n",
    "        pos = {\"VERB\":0, \"NOUN\":0, \"PRON\":0, \"ADJ\":0, \"ADV\":0, \"AUX\":0,\n",
    "               \"PROPN\":0, \"NUM\":0, \"PUNCT\":0}\n",
    "        for token in doc:\n",
    "            if token.pos_ in pos:\n",
    "                pos[token.pos_] += 1\n",
    "        vector_values = list(pos.values()) # convert dict values to list\n",
    "        vector = vector + vector_values # concatenate lists\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0d6ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "[38, 3, 8, 1, 5, 1, 3, 2, 0, 5]\n"
     ]
    }
   ],
   "source": [
    "sample_text = train_dataset[0]['text']\n",
    "vectorizer = POS_vectorizer(small_model)\n",
    "vector = vectorizer.vectorize(sample_text)\n",
    "print(sample_text)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59bd106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up training and test matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df = train_dataset.to_pandas() # two columns: text, label\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True) # shuffle the dataframe\n",
    "test_df = test_dataset.to_pandas()\n",
    "\n",
    "train_df[\"vector\"] = train_df[\"text\"].apply(lambda x: vectorizer.vectorize(x))\n",
    "X_train = np.array(train_df[\"vector\"].to_list())\n",
    "y_train = np.array(train_df[\"label\"].to_list())\n",
    "test_df[\"vector\"] = test_df[\"text\"].apply(lambda x: vectorizer.vectorize(x))\n",
    "X_test = np.array(test_df[\"vector\"].to_list())\n",
    "y_test = np.array(test_df[\"label\"].to_list())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad79b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2560, 10), (2560,), (320, 10), (320,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bc64377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.56      0.57       160\n",
      "           1       0.58      0.59      0.58       160\n",
      "\n",
      "    accuracy                           0.58       320\n",
      "   macro avg       0.58      0.58      0.58       320\n",
      "weighted avg       0.58      0.58      0.58       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a classification model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "model = LogisticRegression(C=0.1, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ff5324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW vectorizer\n",
    "%run -i \"../util/util_simple_classifier.ipynb\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53c47ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 39134 stored elements and shape (2560, 8856)>\n",
      "  Coords\tValues\n",
      "  (0, 6578)\t1\n",
      "  (0, 4219)\t1\n",
      "  (0, 2106)\t1\n",
      "  (0, 8000)\t2\n",
      "  (0, 717)\t1\n",
      "  (0, 42)\t1\n",
      "  (0, 1280)\t1\n",
      "  (0, 5260)\t1\n",
      "  (0, 1607)\t1\n",
      "  (0, 7889)\t1\n",
      "  (0, 3630)\t1\n",
      "  (0, 3406)\t1\n",
      "  (0, 4759)\t1\n",
      "  (0, 7345)\t1\n",
      "  (0, 2707)\t1\n",
      "  (0, 3476)\t1\n",
      "  (0, 7883)\t1\n",
      "  (0, 487)\t1\n",
      "  (0, 6769)\t1\n",
      "  (0, 4269)\t1\n",
      "  (0, 1441)\t1\n",
      "  (0, 8405)\t1\n",
      "  (0, 1889)\t1\n",
      "  (0, 5466)\t1\n",
      "  (0, 7461)\t1\n",
      "  :\t:\n",
      "  (2557, 5905)\t1\n",
      "  (2557, 3595)\t1\n",
      "  (2557, 6745)\t1\n",
      "  (2557, 1023)\t1\n",
      "  (2557, 5259)\t1\n",
      "  (2558, 8000)\t1\n",
      "  (2558, 5331)\t1\n",
      "  (2558, 6733)\t1\n",
      "  (2558, 970)\t1\n",
      "  (2558, 4150)\t1\n",
      "  (2558, 874)\t1\n",
      "  (2559, 4219)\t1\n",
      "  (2559, 5128)\t1\n",
      "  (2559, 285)\t1\n",
      "  (2559, 5292)\t1\n",
      "  (2559, 6278)\t1\n",
      "  (2559, 5326)\t1\n",
      "  (2559, 7929)\t1\n",
      "  (2559, 3336)\t1\n",
      "  (2559, 5622)\t1\n",
      "  (2559, 6748)\t1\n",
      "  (2559, 8629)\t1\n",
      "  (2559, 7915)\t1\n",
      "  (2559, 1679)\t1\n",
      "  (2559, 5904)\t1\n"
     ]
    }
   ],
   "source": [
    "(train_df, text_df) = load_train_test_dataset_pd()\n",
    "vectorizer = CountVectorizer(max_df=0.4)\n",
    "X = vectorizer.fit_transform(train_df[\"text\"])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3927bf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "dense_matrix = X.todense()\n",
    "print(dense_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a65ad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10' '100' '101' ... 'zone' 'ótimo' 'últimos']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "261f52cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8856\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f0240",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.stop_words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d298772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
