{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd5b8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN nsubj\n",
      "startup VERB ccomp\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a966c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple is looking at buying U.K. startup for $1 billion"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071615c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp('\"Let\\'s go to N.Y.!\"')\n",
    "for token in doc1:\n",
    "    print(token.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a93656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'symbol'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('SYM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5965a0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c292bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3525915  -0.5595864   1.3269742  -0.4591393  -0.2378951   0.35304713\n",
      "  0.9810033   0.9120197  -0.7636418   0.53343236  0.7111834  -0.4101859\n",
      " -0.22609742  0.21238515  0.1977409   0.5262209  -1.1578097  -0.6223494\n",
      "  0.04481605 -0.9658006   0.11590984  1.0158504  -0.81331897 -0.06708208\n",
      "  0.46827182  0.55026615  1.0141313   0.18985981 -0.32852772  1.5085963\n",
      " -0.68311155  0.2897485  -0.17085919  0.51104367 -0.41968206 -0.44074327\n",
      " -0.00220814  0.44290805  0.10646416  0.26499194 -0.5972402  -0.17584111\n",
      "  0.09915334  1.245649   -0.0993301   0.03572413 -0.94779813 -0.7312204\n",
      " -0.3475603  -1.3627207   0.16587363 -0.22425665  0.8468491  -1.3706408\n",
      "  1.0920432  -0.5718084   0.7467524  -0.34398675 -0.24239466  0.1408653\n",
      " -0.72588027  0.11295792  0.13626975 -0.8679844   0.21890712 -0.10783374\n",
      "  0.71108097  0.06787673 -0.3360198  -0.547363    0.11034714  0.55276895\n",
      "  0.34893125  0.79479325  0.1372495  -0.5995597  -0.12229609 -0.49462122\n",
      " -0.17136315  0.2365407  -0.7538129  -0.51677805 -0.87205154  0.19488558\n",
      "  1.0055629  -0.06117523  0.5568132   0.17665946 -1.0965905   0.5974612\n",
      " -1.401958    0.93490857  1.0865647   0.2098248  -0.6814435   0.43522495]\n"
     ]
    }
   ],
   "source": [
    "word = nlp(\"intelligence\")\n",
    "print(word.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3854f949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.312396882270653\n"
     ]
    }
   ],
   "source": [
    "print(word.vector_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a615429c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, great, another large language model. <-> Another large language? Just what I needed! 0.5156556367874146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1182115/1505322140.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"Oh, great, another large language model.\")\n",
    "doc2 = nlp(\"Another large language? Just what I needed!\")\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4936206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fcdecda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11044490816763727375\n",
      "intelligence\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The impact of artificial intelligence on society is profound.\")\n",
    "print(doc.vocab.strings['intelligence']) # 11044490816763727375\n",
    "print(doc.vocab.strings[11044490816763727375]) # intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "811a4423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4690420944186131903 X I I True False True en\n",
      "love 3702023516439754181 xxxx l ove True False False en\n",
      "coffee 3197928453018144401 xxxx c fee True False False en\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
    "            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf7b22da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'Real', 'Python', 'tutorial', 'on', 'spaCy', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro = \"This is a Real Python tutorial on spaCy.\"\n",
    "intro_doc = nlp(intro)\n",
    "[token.text for token in intro_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6373895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "476055af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(intro_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb6efc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "file_name = pathlib.Path(\"Data/nvidia2025q3.txt\")\n",
    "text = file_name.read_text(encoding=\"utf-8\")\n",
    "nvidia_doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fe84676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nvidia',\n",
       " '(',\n",
       " '\\n',\n",
       " 'NVDA',\n",
       " '\\n',\n",
       " '+2.61',\n",
       " '%',\n",
       " '\\n',\n",
       " ')',\n",
       " '\\n',\n",
       " 'Q3',\n",
       " '2025',\n",
       " 'Earnings',\n",
       " 'Call',\n",
       " '\\n',\n",
       " 'Nov',\n",
       " '20',\n",
       " ',',\n",
       " '2024',\n",
       " ',',\n",
       " '5:00',\n",
       " 'p.m.',\n",
       " 'ET',\n",
       " '\\n\\n',\n",
       " 'Contents',\n",
       " ':',\n",
       " '\\n',\n",
       " 'Prepared',\n",
       " 'Remarks',\n",
       " '\\n',\n",
       " 'Questions',\n",
       " 'and',\n",
       " 'Answers',\n",
       " '\\n',\n",
       " 'Call',\n",
       " 'Participants',\n",
       " '\\n',\n",
       " 'Prepared',\n",
       " 'Remarks',\n",
       " ':',\n",
       " '\\n\\n',\n",
       " 'Operator',\n",
       " '\\n\\n',\n",
       " 'Good',\n",
       " 'afternoon',\n",
       " '.',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Jay',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'be',\n",
       " 'your',\n",
       " 'conference',\n",
       " 'operator',\n",
       " 'today',\n",
       " '.',\n",
       " 'At',\n",
       " 'this',\n",
       " 'time',\n",
       " ',',\n",
       " 'I',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'welcome',\n",
       " 'everyone',\n",
       " 'to',\n",
       " 'NVIDIA',\n",
       " \"'s\",\n",
       " 'third',\n",
       " '-',\n",
       " 'quarter',\n",
       " 'earnings',\n",
       " 'call',\n",
       " '.',\n",
       " 'All',\n",
       " 'lines',\n",
       " 'have',\n",
       " 'been',\n",
       " 'placed',\n",
       " 'on',\n",
       " 'mute',\n",
       " 'to',\n",
       " 'prevent',\n",
       " 'any',\n",
       " 'background',\n",
       " 'noise',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'After',\n",
       " 'the',\n",
       " 'speakers',\n",
       " \"'\",\n",
       " 'remarks',\n",
       " ',',\n",
       " 'there',\n",
       " 'will',\n",
       " 'be',\n",
       " 'a',\n",
       " 'question',\n",
       " '-',\n",
       " 'and',\n",
       " '-',\n",
       " 'answer',\n",
       " 'session',\n",
       " '.',\n",
       " '[',\n",
       " 'Operator',\n",
       " 'instructions',\n",
       " ']',\n",
       " 'Thank',\n",
       " 'you',\n",
       " '.',\n",
       " 'Stewart',\n",
       " 'Stecker',\n",
       " ',',\n",
       " 'you',\n",
       " 'may',\n",
       " 'begin',\n",
       " 'your',\n",
       " 'conference',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'Stewart',\n",
       " 'Stecker',\n",
       " '--',\n",
       " 'Senior',\n",
       " 'Director',\n",
       " ',',\n",
       " 'Investor',\n",
       " 'Relations',\n",
       " '\\n\\n',\n",
       " 'Thank',\n",
       " 'you',\n",
       " '.',\n",
       " 'Good',\n",
       " 'afternoon',\n",
       " ',',\n",
       " 'everyone',\n",
       " ',',\n",
       " 'and',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'NVIDIA',\n",
       " \"'s\",\n",
       " 'conference',\n",
       " 'call',\n",
       " 'for',\n",
       " 'the',\n",
       " 'third',\n",
       " 'quarter',\n",
       " 'of',\n",
       " 'fiscal',\n",
       " '2025',\n",
       " '.',\n",
       " 'With',\n",
       " 'me',\n",
       " 'today',\n",
       " 'from',\n",
       " 'NVIDIA',\n",
       " 'are',\n",
       " 'Jensen',\n",
       " 'Huang',\n",
       " ',',\n",
       " 'president',\n",
       " 'and',\n",
       " 'chief',\n",
       " 'executive',\n",
       " 'officer',\n",
       " ';',\n",
       " 'and',\n",
       " 'Colette',\n",
       " 'Kress',\n",
       " ',',\n",
       " 'executive',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'and',\n",
       " 'chief',\n",
       " 'financial',\n",
       " 'officer',\n",
       " '.',\n",
       " 'I',\n",
       " \"'d\",\n",
       " 'like',\n",
       " 'to',\n",
       " 'remind',\n",
       " 'you',\n",
       " 'that',\n",
       " 'our',\n",
       " 'call',\n",
       " 'is',\n",
       " 'being',\n",
       " 'webcast',\n",
       " 'live',\n",
       " 'on',\n",
       " 'NVIDIA',\n",
       " \"'s\",\n",
       " 'Investor',\n",
       " 'Relations',\n",
       " 'website',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'The',\n",
       " 'webcast',\n",
       " 'will',\n",
       " 'be',\n",
       " 'available',\n",
       " 'for',\n",
       " 'replay',\n",
       " 'until',\n",
       " 'the',\n",
       " 'conference',\n",
       " 'call',\n",
       " 'to',\n",
       " 'discuss',\n",
       " 'our',\n",
       " 'financial',\n",
       " 'results',\n",
       " 'for',\n",
       " 'the',\n",
       " 'fourth',\n",
       " 'quarter',\n",
       " 'of',\n",
       " 'fiscal',\n",
       " '2025',\n",
       " '.',\n",
       " 'The',\n",
       " 'content',\n",
       " 'of',\n",
       " 'today',\n",
       " \"'s\",\n",
       " 'call',\n",
       " 'is',\n",
       " 'NVIDIA',\n",
       " \"'s\",\n",
       " 'property',\n",
       " '.',\n",
       " 'It',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'be',\n",
       " 'reproduced',\n",
       " 'or',\n",
       " 'transcribed',\n",
       " 'without',\n",
       " 'our',\n",
       " 'prior',\n",
       " 'written',\n",
       " 'consent',\n",
       " '.',\n",
       " 'During',\n",
       " 'this',\n",
       " 'call',\n",
       " ',',\n",
       " 'we',\n",
       " 'may',\n",
       " 'make',\n",
       " 'forward',\n",
       " '-',\n",
       " 'looking',\n",
       " 'statements',\n",
       " 'based',\n",
       " 'on',\n",
       " 'current',\n",
       " 'expectations',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'These',\n",
       " 'are',\n",
       " 'subject',\n",
       " 'to',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'significant',\n",
       " 'risks',\n",
       " 'and',\n",
       " 'uncertainties',\n",
       " ',',\n",
       " 'and',\n",
       " 'our',\n",
       " 'actual',\n",
       " 'results',\n",
       " 'may',\n",
       " 'differ',\n",
       " 'materially',\n",
       " '.',\n",
       " 'For',\n",
       " 'a',\n",
       " 'discussion',\n",
       " 'of',\n",
       " 'factors',\n",
       " 'that',\n",
       " 'could',\n",
       " 'affect',\n",
       " 'our',\n",
       " 'future',\n",
       " 'financial',\n",
       " 'results',\n",
       " 'and',\n",
       " 'business',\n",
       " ',',\n",
       " 'please',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'disclosure',\n",
       " 'in',\n",
       " 'today',\n",
       " \"'s\",\n",
       " 'earnings',\n",
       " 'release',\n",
       " ',',\n",
       " 'our',\n",
       " 'most',\n",
       " 'recent',\n",
       " 'Forms',\n",
       " '10',\n",
       " '-',\n",
       " 'K',\n",
       " 'and',\n",
       " '10',\n",
       " '-',\n",
       " 'Q',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'reports',\n",
       " 'that',\n",
       " 'we',\n",
       " 'may',\n",
       " 'file',\n",
       " 'on',\n",
       " 'Form',\n",
       " '8',\n",
       " '-',\n",
       " 'K',\n",
       " 'with',\n",
       " 'the',\n",
       " 'Securities',\n",
       " 'and',\n",
       " 'Exchange',\n",
       " 'Commission',\n",
       " '.',\n",
       " 'All',\n",
       " 'our',\n",
       " 'statements',\n",
       " 'are',\n",
       " 'made',\n",
       " 'as',\n",
       " 'of',\n",
       " 'today',\n",
       " ',',\n",
       " 'November',\n",
       " '20',\n",
       " ',',\n",
       " '2024',\n",
       " ',',\n",
       " 'based',\n",
       " 'on',\n",
       " 'information',\n",
       " 'currently',\n",
       " 'available',\n",
       " 'to',\n",
       " 'us',\n",
       " '.',\n",
       " 'Except',\n",
       " 'as',\n",
       " 'required',\n",
       " 'by',\n",
       " 'law',\n",
       " ',',\n",
       " 'we',\n",
       " 'assume',\n",
       " 'no',\n",
       " 'obligation',\n",
       " 'to',\n",
       " 'update',\n",
       " 'any',\n",
       " 'such',\n",
       " 'statements',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'During',\n",
       " 'this',\n",
       " 'call',\n",
       " ',',\n",
       " 'we',\n",
       " 'will',\n",
       " 'discuss',\n",
       " 'non',\n",
       " '-',\n",
       " 'GAAP',\n",
       " 'financial',\n",
       " 'measures',\n",
       " '.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'find',\n",
       " 'a',\n",
       " 'reconciliation',\n",
       " 'of',\n",
       " 'these',\n",
       " 'non',\n",
       " '-',\n",
       " 'GAAP',\n",
       " 'financial',\n",
       " 'measures',\n",
       " 'to',\n",
       " 'GAAP',\n",
       " 'financial',\n",
       " 'measures',\n",
       " 'in',\n",
       " 'our',\n",
       " 'CFO',\n",
       " 'commentary',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'posted',\n",
       " 'on',\n",
       " 'our',\n",
       " 'website',\n",
       " '.',\n",
       " 'With',\n",
       " 'that',\n",
       " ',',\n",
       " 'let',\n",
       " 'me',\n",
       " 'turn',\n",
       " 'the',\n",
       " 'call',\n",
       " 'over',\n",
       " 'to',\n",
       " 'Colette',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'Colette',\n",
       " 'M.',\n",
       " 'Kress',\n",
       " '--',\n",
       " 'Chief',\n",
       " 'Financial',\n",
       " 'Officer',\n",
       " ',',\n",
       " 'Executive',\n",
       " 'Vice',\n",
       " 'President',\n",
       " '\\n\\n',\n",
       " 'Thank',\n",
       " 'you',\n",
       " ',',\n",
       " 'Stewart',\n",
       " '.',\n",
       " 'Q3',\n",
       " 'was',\n",
       " 'another',\n",
       " 'record',\n",
       " 'quarter',\n",
       " '.',\n",
       " 'We',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'deliver',\n",
       " 'incredible',\n",
       " 'growth',\n",
       " '.',\n",
       " 'Revenue',\n",
       " 'of',\n",
       " '$',\n",
       " '35.1',\n",
       " 'billion',\n",
       " 'was',\n",
       " 'up',\n",
       " '17',\n",
       " '%',\n",
       " 'sequentially',\n",
       " 'and',\n",
       " 'up',\n",
       " '94',\n",
       " '%',\n",
       " 'year',\n",
       " 'on',\n",
       " 'year',\n",
       " 'and',\n",
       " 'well',\n",
       " 'above',\n",
       " 'our',\n",
       " 'outlook',\n",
       " 'of',\n",
       " '$',\n",
       " '32.5',\n",
       " 'billion',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'All',\n",
       " 'market',\n",
       " 'platforms',\n",
       " 'posted',\n",
       " 'strong',\n",
       " 'sequential',\n",
       " 'and',\n",
       " 'year',\n",
       " '-',\n",
       " 'over',\n",
       " '-',\n",
       " 'year',\n",
       " 'growth',\n",
       " ',',\n",
       " 'fueled',\n",
       " 'by',\n",
       " 'the',\n",
       " 'adoption',\n",
       " 'of',\n",
       " 'NVIDIA',\n",
       " 'accelerated',\n",
       " 'computing',\n",
       " 'and',\n",
       " 'AI',\n",
       " '.',\n",
       " 'Starting',\n",
       " 'with',\n",
       " 'data',\n",
       " 'center',\n",
       " '.',\n",
       " 'Another',\n",
       " 'record',\n",
       " 'was',\n",
       " 'achieved',\n",
       " 'in',\n",
       " 'data',\n",
       " 'center',\n",
       " '.',\n",
       " 'Revenue',\n",
       " 'of',\n",
       " '$',\n",
       " '30.8',\n",
       " 'billion',\n",
       " ',',\n",
       " 'up',\n",
       " '17',\n",
       " '%',\n",
       " 'sequential',\n",
       " 'and',\n",
       " 'up',\n",
       " '112',\n",
       " '%',\n",
       " 'year',\n",
       " 'on',\n",
       " 'year',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'NVIDIA',\n",
       " 'Hopper',\n",
       " 'demand',\n",
       " 'is',\n",
       " 'exceptional',\n",
       " ',',\n",
       " 'and',\n",
       " 'sequentially',\n",
       " ',',\n",
       " 'NVIDIA',\n",
       " 'H200',\n",
       " 'sales',\n",
       " 'increased',\n",
       " 'significantly',\n",
       " 'to',\n",
       " 'double',\n",
       " '-',\n",
       " 'digit',\n",
       " 'billions',\n",
       " ',',\n",
       " 'the',\n",
       " 'fastest',\n",
       " 'prod',\n",
       " 'ramp',\n",
       " 'in',\n",
       " 'our',\n",
       " 'company',\n",
       " \"'s\",\n",
       " 'history',\n",
       " '.',\n",
       " 'The',\n",
       " 'H200',\n",
       " 'delivers',\n",
       " 'up',\n",
       " 'to',\n",
       " '2x',\n",
       " 'faster',\n",
       " 'inference',\n",
       " 'performance',\n",
       " 'and',\n",
       " 'up',\n",
       " 'to',\n",
       " '50',\n",
       " '%',\n",
       " 'improved',\n",
       " 'TCO',\n",
       " '.',\n",
       " 'Cloud',\n",
       " 'service',\n",
       " 'providers',\n",
       " 'were',\n",
       " 'approximately',\n",
       " 'half',\n",
       " 'of',\n",
       " 'our',\n",
       " 'data',\n",
       " 'center',\n",
       " 'sales',\n",
       " 'with',\n",
       " 'revenue',\n",
       " 'increasing',\n",
       " 'more',\n",
       " 'than',\n",
       " '2x',\n",
       " 'year',\n",
       " 'on',\n",
       " 'year',\n",
       " '.',\n",
       " 'CSPs',\n",
       " 'deployed',\n",
       " 'NVIDIA',\n",
       " 'H200',\n",
       " 'infrastructure',\n",
       " 'and',\n",
       " 'high',\n",
       " '-',\n",
       " 'speed',\n",
       " 'networking',\n",
       " 'with',\n",
       " 'installations',\n",
       " 'scaling',\n",
       " 'to',\n",
       " 'tens',\n",
       " 'of',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'DPUs',\n",
       " 'to',\n",
       " 'grow',\n",
       " 'their',\n",
       " 'business',\n",
       " 'and',\n",
       " 'serve',\n",
       " 'rapidly',\n",
       " 'rising',\n",
       " 'demand',\n",
       " 'for',\n",
       " 'AI',\n",
       " 'training',\n",
       " 'and',\n",
       " 'inference',\n",
       " 'workloads',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'NVIDIA',\n",
       " 'H200',\n",
       " '-',\n",
       " 'powered',\n",
       " 'cloud',\n",
       " 'instances',\n",
       " 'are',\n",
       " 'now',\n",
       " 'available',\n",
       " 'from',\n",
       " 'AWS',\n",
       " ',',\n",
       " 'CoreWeave',\n",
       " ',',\n",
       " 'and',\n",
       " 'Microsoft',\n",
       " 'Azure',\n",
       " ',',\n",
       " 'with',\n",
       " 'Google',\n",
       " 'Cloud',\n",
       " 'and',\n",
       " 'OCI',\n",
       " 'coming',\n",
       " 'soon',\n",
       " '.',\n",
       " 'Alongside',\n",
       " 'significant',\n",
       " 'growth',\n",
       " 'from',\n",
       " 'our',\n",
       " 'large',\n",
       " 'CSPs',\n",
       " ',',\n",
       " 'NVIDIA',\n",
       " 'GPU',\n",
       " 'regional',\n",
       " 'cloud',\n",
       " 'revenue',\n",
       " 'jumped',\n",
       " 'year',\n",
       " 'on',\n",
       " 'year',\n",
       " 'as',\n",
       " 'North',\n",
       " 'America',\n",
       " ',',\n",
       " 'India',\n",
       " ',',\n",
       " 'and',\n",
       " 'Asia',\n",
       " 'Pacific',\n",
       " 'regions',\n",
       " 'ramped',\n",
       " 'NVIDIA',\n",
       " 'Cloud',\n",
       " 'instances',\n",
       " 'and',\n",
       " 'sovereign',\n",
       " 'cloud',\n",
       " 'build',\n",
       " '-',\n",
       " 'outs',\n",
       " '.',\n",
       " 'Consumer',\n",
       " 'Internet',\n",
       " 'revenue',\n",
       " 'more',\n",
       " 'than',\n",
       " 'doubled',\n",
       " 'year',\n",
       " 'on',\n",
       " 'year',\n",
       " 'as',\n",
       " 'companies',\n",
       " 'scaled',\n",
       " 'their',\n",
       " 'NVIDIA',\n",
       " 'Hopper',\n",
       " 'infrastructure',\n",
       " 'to',\n",
       " 'support',\n",
       " 'next',\n",
       " '-',\n",
       " 'generation',\n",
       " 'AI',\n",
       " 'models',\n",
       " 'training',\n",
       " ',',\n",
       " 'multimodal',\n",
       " ',',\n",
       " 'and',\n",
       " 'agentic',\n",
       " 'AI',\n",
       " ',',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'recommender',\n",
       " 'engines',\n",
       " ',',\n",
       " 'and',\n",
       " 'generative',\n",
       " 'AI',\n",
       " 'inference',\n",
       " 'and',\n",
       " 'content',\n",
       " 'creation',\n",
       " 'workloads',\n",
       " '.',\n",
       " 'NVIDIA',\n",
       " 'Ampere',\n",
       " 'and',\n",
       " 'Hopper',\n",
       " 'infrastructures',\n",
       " 'are',\n",
       " 'fueling',\n",
       " 'inference',\n",
       " 'revenue',\n",
       " 'growth',\n",
       " 'for',\n",
       " 'customers',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'NVIDIA',\n",
       " 'is',\n",
       " 'the',\n",
       " 'largest',\n",
       " 'inference',\n",
       " 'platform',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'Our',\n",
       " 'large',\n",
       " 'installed',\n",
       " 'base',\n",
       " 'and',\n",
       " 'rich',\n",
       " 'software',\n",
       " 'ecosystem',\n",
       " 'encourage',\n",
       " 'developers',\n",
       " 'to',\n",
       " 'optimize',\n",
       " 'for',\n",
       " 'NVIDIA',\n",
       " 'and',\n",
       " 'deliver',\n",
       " 'continued',\n",
       " 'performance',\n",
       " 'and',\n",
       " 'TCO',\n",
       " 'improvements',\n",
       " '.',\n",
       " 'Rapid',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'NVIDIA',\n",
       " 'software',\n",
       " 'algorithms',\n",
       " 'boosted',\n",
       " 'Hopper',\n",
       " 'inference',\n",
       " 'throughput',\n",
       " 'by',\n",
       " 'an',\n",
       " 'incredible',\n",
       " '5x',\n",
       " 'in',\n",
       " 'one',\n",
       " 'year',\n",
       " 'and',\n",
       " 'cut',\n",
       " 'time',\n",
       " 'to',\n",
       " 'first',\n",
       " 'token',\n",
       " 'by',\n",
       " '5x',\n",
       " '.',\n",
       " 'Our',\n",
       " 'upcoming',\n",
       " 'release',\n",
       " 'of',\n",
       " 'NVIDIA',\n",
       " 'NIM',\n",
       " 'will',\n",
       " 'boost',\n",
       " 'Hopper',\n",
       " 'inference',\n",
       " 'performance',\n",
       " 'by',\n",
       " 'an',\n",
       " 'additional',\n",
       " '2.4x',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'Continuous',\n",
       " 'performance',\n",
       " 'optimizations',\n",
       " 'are',\n",
       " 'a',\n",
       " 'hallmark',\n",
       " 'of',\n",
       " 'NVIDIA',\n",
       " 'and',\n",
       " 'drive',\n",
       " 'increasingly',\n",
       " 'economic',\n",
       " 'returns',\n",
       " 'for',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'NVIDIA',\n",
       " 'installed',\n",
       " 'base',\n",
       " '.',\n",
       " 'Blackwell',\n",
       " 'is',\n",
       " 'in',\n",
       " 'full',\n",
       " 'production',\n",
       " 'after',\n",
       " 'a',\n",
       " 'successfully',\n",
       " 'executed',\n",
       " 'change',\n",
       " '.',\n",
       " 'We',\n",
       " 'shipped',\n",
       " '13,000',\n",
       " 'GPU',\n",
       " 'samples',\n",
       " 'to',\n",
       " 'customers',\n",
       " 'in',\n",
       " 'the',\n",
       " 'third',\n",
       " 'quarter',\n",
       " ',',\n",
       " 'including',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'Blackwell',\n",
       " 'DGX',\n",
       " 'engineering',\n",
       " 'samples',\n",
       " 'to',\n",
       " 'OpenAI',\n",
       " '.',\n",
       " 'Blackwell',\n",
       " 'is',\n",
       " 'a',\n",
       " 'full',\n",
       " '-',\n",
       " 'stack',\n",
       " ',',\n",
       " 'full',\n",
       " '-',\n",
       " 'infrastructure',\n",
       " ',',\n",
       " 'AI',\n",
       " 'data',\n",
       " 'center',\n",
       " 'scale',\n",
       " 'system',\n",
       " 'with',\n",
       " 'customizable',\n",
       " 'configurations',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'address',\n",
       " 'a',\n",
       " 'diverse',\n",
       " 'and',\n",
       " 'growing',\n",
       " 'AI',\n",
       " 'market',\n",
       " 'from',\n",
       " 'x86',\n",
       " 'to',\n",
       " 'ARM',\n",
       " ',',\n",
       " 'training',\n",
       " 'to',\n",
       " 'inferencing',\n",
       " 'GPUs',\n",
       " ',',\n",
       " 'InfiniBand',\n",
       " 'to',\n",
       " 'Ethernet',\n",
       " 'switches',\n",
       " ',',\n",
       " 'and',\n",
       " 'NVLink',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'And',\n",
       " 'from',\n",
       " 'liquid',\n",
       " '-',\n",
       " 'cooled',\n",
       " 'to',\n",
       " 'air',\n",
       " '-',\n",
       " 'cooled',\n",
       " ',',\n",
       " 'every',\n",
       " 'customer',\n",
       " 'is',\n",
       " 'racing',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " 'first',\n",
       " 'to',\n",
       " 'market',\n",
       " '.',\n",
       " 'Blackwell',\n",
       " 'is',\n",
       " 'now',\n",
       " 'in',\n",
       " 'the',\n",
       " 'hands',\n",
       " 'of',\n",
       " 'all',\n",
       " 'of',\n",
       " 'our',\n",
       " 'major',\n",
       " 'partners',\n",
       " ',',\n",
       " 'and',\n",
       " 'they',\n",
       " 'are',\n",
       " 'working',\n",
       " 'to',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in nvidia_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8f45cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all sentences and put them in a list\n",
    "sentences = list(nvidia_doc.sents)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "938d5ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to remind you that our\n",
      "The webcast will be available for replay until\n",
      "The content of today's call is NVIDIA\n",
      "It can't be reproduced or transcribed without\n",
      "During this call, we may make forward\n"
     ]
    }
   ],
   "source": [
    "# print the first few tokens for selected sentences\n",
    "for sentence in sentences[10:15]:\n",
    "    print(sentence[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fbf207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "ellipsis_text = (\n",
    "    \"Guys, can you please, ... never mind, I forgot\"\n",
    "    \"what I was saying. So, do you think\"\n",
    "    \" we should...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8737d841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guys, can you please, ...\n",
      "never mind, I forgotwhat I was saying.\n",
      "So, do you think we should...\n"
     ]
    }
   ],
   "source": [
    "# Define a custom delimiter to break up sentences\n",
    "from spacy.language import Language\n",
    "\n",
    "# Decorator\n",
    "@Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \"...\":\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "# Custom nlp object\n",
    "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    "custom_nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
    "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
    "for sentence in custom_ellipsis_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "544fc3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guys 0\n",
      ", 4\n",
      "can 6\n",
      "you 10\n",
      "please 14\n",
      ", 20\n",
      "... 22\n",
      "never 26\n",
      "mind 32\n",
      ", 36\n",
      "I 38\n",
      "forgotwhat 40\n",
      "I 51\n",
      "was 53\n",
      "saying 57\n",
      ". 63\n",
      "So 65\n",
      ", 67\n",
      "do 69\n",
      "you 72\n",
      "think 76\n",
      "we 82\n",
      "should 85\n",
      "... 91\n"
     ]
    }
   ],
   "source": [
    "for token in custom_ellipsis_doc:\n",
    "    print(token, token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8348e109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with Whitespace  Is Alphanumeric?Is Punctuation?   Is Stop Word?\n",
      "Guys                  True           False             False\n",
      ",                     False          True              False\n",
      "can                   True           False             True\n",
      "you                   True           False             True\n",
      "please                True           False             True\n",
      ",                     False          True              False\n",
      "...                   False          True              False\n",
      "never                 True           False             True\n",
      "mind                  True           False             False\n",
      ",                     False          True              False\n",
      "I                     True           False             True\n",
      "forgotwhat            True           False             False\n",
      "I                     True           False             True\n",
      "was                   True           False             True\n",
      "saying                True           False             False\n",
      ".                     False          True              False\n",
      "So                    True           False             True\n",
      ",                     False          True              False\n",
      "do                    True           False             True\n",
      "you                   True           False             True\n",
      "think                 True           False             False\n",
      "we                    True           False             True\n",
      "should                True           False             True\n",
      "...                   False          True              False\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"{'Text with Whitespace':22}\"\n",
    "    f\"{'Is Alphanumeric?':15}\"\n",
    "    f\"{'Is Punctuation?':18}\"\n",
    "    f\"{'Is Stop Word?'}\"\n",
    ")\n",
    "\n",
    "for token in custom_ellipsis_doc:\n",
    "    print((\n",
    "        f\"{str(token.text_with_ws):22}\"\n",
    "        f\"{str(token.is_alpha):15}\"\n",
    "        f\"{str(token.is_punct):18}\"\n",
    "        f\"{str(token.is_stop)}\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9143d3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'London@based', 'FinTech', 'company', '.']\n"
     ]
    }
   ],
   "source": [
    "# Custom text with @infix\n",
    "custom_about_text = (\n",
    "    \"This is a London@based FinTech company.\"\n",
    ")\n",
    "\n",
    "print([token.text for token in nlp(custom_about_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35a3f1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'London', '@', 'based', 'FinTech', 'company', '.']\n"
     ]
    }
   ],
   "source": [
    "# Use default prefix, suffix, but customize infix\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    "prefix_re = spacy.util.compile_prefix_regex(\n",
    "    custom_nlp.Defaults.prefixes\n",
    ")\n",
    "suffix_re = spacy.util.compile_suffix_regex(\n",
    "    custom_nlp.Defaults.suffixes\n",
    ")\n",
    "custom_infixes = [f\"@\"]\n",
    "infix_re = spacy.util.compile_infix_regex(\n",
    "    list(custom_nlp.Defaults.infixes) + custom_infixes\n",
    ")\n",
    "\n",
    "custom_nlp.tokenizer = Tokenizer(\n",
    "    custom_nlp.vocab,\n",
    "    prefix_search=prefix_re.search,\n",
    "    suffix_search=suffix_re.search,\n",
    "    infix_finditer=infix_re.finditer,\n",
    "    token_match=None\n",
    ")\n",
    "\n",
    "custom_tokenizer_about_doc = custom_nlp(custom_about_text)\n",
    "print([token.text for token in custom_tokenizer_about_doc])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "051f6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "# Load the default English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add \"@\" as a custom infix (split inside tokens)\n",
    "custom_infixes = list(nlp.Defaults.infixes) + [r\"@\"]\n",
    "infix_re = compile_infix_regex(custom_infixes)\n",
    "\n",
    "# Create a new tokenizer that only overrides the infix rules\n",
    "nlp.tokenizer = Tokenizer(\n",
    "    nlp.vocab,\n",
    "    prefix_search=nlp.tokenizer.prefix_search,\n",
    "    suffix_search=nlp.tokenizer.suffix_search,\n",
    "    infix_finditer=infix_re.finditer,\n",
    "    token_match=nlp.tokenizer.token_match,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3eefbfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'London', '@', 'based', 'FinTech', 'company', '.']\n"
     ]
    }
   ],
   "source": [
    "custom_tokenizer_about_doc = nlp(custom_about_text)\n",
    "print([token.text for token in custom_tokenizer_about_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b07e5f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b0894d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first\n",
      "next\n",
      "even\n",
      "thence\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "for word in list(spacy_stopwords)[:5]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4736171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Guys, ,, ,, ..., mind, ,, forgotwhat, saying, ., ,, think, ...]\n"
     ]
    }
   ],
   "source": [
    "print([token for token in custom_ellipsis_doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "849dc7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Guys: guy\n",
      "                 was: be\n",
      "              saying: say\n",
      "                  So: so\n"
     ]
    }
   ],
   "source": [
    "for token in custom_ellipsis_doc:\n",
    "    if str(token) != str(token.lemma_):\n",
    "        print(f\"{str(token):>20}: {str(token.lemma_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9510c9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AI', 109), ('NVIDIA', 59), ('Blackwell', 49), ('year', 43), ('data', 43)]\n"
     ]
    }
   ],
   "source": [
    "# Counting for word frequencies, excl stop words, punctuation, and line breaks\n",
    "from collections import Counter\n",
    "\n",
    "words_freq = [\n",
    "    token.text\n",
    "    for token in nvidia_doc\n",
    "    if not token.is_stop and not token.is_punct and not token.is_space\n",
    "]\n",
    "print(Counter(words_freq).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6935ba6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Token: Guys\n",
      "    ==========================\n",
      "    Tag: NNS        POS: NOUN\n",
      "    Explanation: noun, plural\n",
      "    \n",
      "\n",
      "    Token: ,\n",
      "    ==========================\n",
      "    Tag: ,          POS: PUNCT\n",
      "    Explanation: punctuation mark, comma\n",
      "    \n",
      "\n",
      "    Token: can\n",
      "    ==========================\n",
      "    Tag: MD         POS: AUX\n",
      "    Explanation: verb, modal auxiliary\n",
      "    \n",
      "\n",
      "    Token: you\n",
      "    ==========================\n",
      "    Tag: PRP        POS: PRON\n",
      "    Explanation: pronoun, personal\n",
      "    \n",
      "\n",
      "    Token: please\n",
      "    ==========================\n",
      "    Tag: VB         POS: VERB\n",
      "    Explanation: verb, base form\n",
      "    \n",
      "\n",
      "    Token: ,\n",
      "    ==========================\n",
      "    Tag: ,          POS: PUNCT\n",
      "    Explanation: punctuation mark, comma\n",
      "    \n",
      "\n",
      "    Token: ...\n",
      "    ==========================\n",
      "    Tag: :          POS: PUNCT\n",
      "    Explanation: punctuation mark, colon or ellipsis\n",
      "    \n",
      "\n",
      "    Token: never\n",
      "    ==========================\n",
      "    Tag: RB         POS: ADV\n",
      "    Explanation: adverb\n",
      "    \n",
      "\n",
      "    Token: mind\n",
      "    ==========================\n",
      "    Tag: VBP        POS: VERB\n",
      "    Explanation: verb, non-3rd person singular present\n",
      "    \n",
      "\n",
      "    Token: ,\n",
      "    ==========================\n",
      "    Tag: ,          POS: PUNCT\n",
      "    Explanation: punctuation mark, comma\n",
      "    \n",
      "\n",
      "    Token: I\n",
      "    ==========================\n",
      "    Tag: PRP        POS: PRON\n",
      "    Explanation: pronoun, personal\n",
      "    \n",
      "\n",
      "    Token: forgotwhat\n",
      "    ==========================\n",
      "    Tag: VBP        POS: VERB\n",
      "    Explanation: verb, non-3rd person singular present\n",
      "    \n",
      "\n",
      "    Token: I\n",
      "    ==========================\n",
      "    Tag: PRP        POS: PRON\n",
      "    Explanation: pronoun, personal\n",
      "    \n",
      "\n",
      "    Token: was\n",
      "    ==========================\n",
      "    Tag: VBD        POS: AUX\n",
      "    Explanation: verb, past tense\n",
      "    \n",
      "\n",
      "    Token: saying\n",
      "    ==========================\n",
      "    Tag: VBG        POS: VERB\n",
      "    Explanation: verb, gerund or present participle\n",
      "    \n",
      "\n",
      "    Token: .\n",
      "    ==========================\n",
      "    Tag: .          POS: PUNCT\n",
      "    Explanation: punctuation mark, sentence closer\n",
      "    \n",
      "\n",
      "    Token: So\n",
      "    ==========================\n",
      "    Tag: RB         POS: ADV\n",
      "    Explanation: adverb\n",
      "    \n",
      "\n",
      "    Token: ,\n",
      "    ==========================\n",
      "    Tag: ,          POS: PUNCT\n",
      "    Explanation: punctuation mark, comma\n",
      "    \n",
      "\n",
      "    Token: do\n",
      "    ==========================\n",
      "    Tag: VBP        POS: AUX\n",
      "    Explanation: verb, non-3rd person singular present\n",
      "    \n",
      "\n",
      "    Token: you\n",
      "    ==========================\n",
      "    Tag: PRP        POS: PRON\n",
      "    Explanation: pronoun, personal\n",
      "    \n",
      "\n",
      "    Token: think\n",
      "    ==========================\n",
      "    Tag: VB         POS: VERB\n",
      "    Explanation: verb, base form\n",
      "    \n",
      "\n",
      "    Token: we\n",
      "    ==========================\n",
      "    Tag: PRP        POS: PRON\n",
      "    Explanation: pronoun, personal\n",
      "    \n",
      "\n",
      "    Token: should\n",
      "    ==========================\n",
      "    Tag: MD         POS: AUX\n",
      "    Explanation: verb, modal auxiliary\n",
      "    \n",
      "\n",
      "    Token: ...\n",
      "    ==========================\n",
      "    Tag: .          POS: PUNCT\n",
      "    Explanation: punctuation mark, sentence closer\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for token in custom_ellipsis_doc:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "    Token: {token.text}\n",
    "    ==========================\n",
    "    Tag: {token.tag_:10} POS: {token.pos_}\n",
    "    Explanation: {spacy.explain(token.tag_)}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "052d6a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nouns: 1819\n",
      "Number of adjectives: 634\n",
      "Most frequent adjectives:\n",
      "new: 33\n",
      "next: 26\n",
      "first: 19\n",
      "more: 17\n",
      "large: 15\n"
     ]
    }
   ],
   "source": [
    "nouns = []\n",
    "adjs = []\n",
    "for token in nvidia_doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        nouns.append(token.text)\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        adjs.append(token.text)\n",
    "print(f\"Number of nouns: {len(nouns)}\")\n",
    "print(f\"Number of adjectives: {len(adjs)}\")\n",
    "\n",
    "# count frequency of adjectives\n",
    "adj_counts = Counter(adjs)\n",
    "\n",
    "# get top five most frequent adjectives\n",
    "top5_adj = adj_counts.most_common(5)\n",
    "print(\"Most frequent adjectives:\")\n",
    "for word, freq in top5_adj:\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6db5ec6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f78e9946f01b47a8837dfedfc143fade-0\" class=\"displacy\" width=\"1580\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Guys,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">can</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">you</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">please, ...</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">never</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">mind,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">forgotwhat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">saying.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">So,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">do</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1220\">you</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1220\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1310\">think</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1310\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">we</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1490\">should...</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1490\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-0\" stroke-width=\"2px\" d=\"M70,182.0 C70,47.0 315.0,47.0 315.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,184.0 L62,172.0 78,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-1\" stroke-width=\"2px\" d=\"M160,182.0 C160,92.0 310.0,92.0 310.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M160,184.0 L152,172.0 168,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-2\" stroke-width=\"2px\" d=\"M250,182.0 C250,137.0 305.0,137.0 305.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M250,184.0 L242,172.0 258,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-3\" stroke-width=\"2px\" d=\"M430,182.0 C430,137.0 485.0,137.0 485.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M430,184.0 L422,172.0 438,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-4\" stroke-width=\"2px\" d=\"M520,182.0 C520,137.0 575.0,137.0 575.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,184.0 L583.0,172.0 567.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-5\" stroke-width=\"2px\" d=\"M700,182.0 C700,47.0 945.0,47.0 945.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M700,184.0 L692,172.0 708,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-6\" stroke-width=\"2px\" d=\"M790,182.0 C790,92.0 940.0,92.0 940.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,184.0 L782,172.0 798,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-7\" stroke-width=\"2px\" d=\"M880,182.0 C880,137.0 935.0,137.0 935.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M880,184.0 L872,172.0 888,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-8\" stroke-width=\"2px\" d=\"M610,182.0 C610,2.0 950.0,2.0 950.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M950.0,184.0 L958.0,172.0 942.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-9\" stroke-width=\"2px\" d=\"M1060,182.0 C1060,47.0 1305.0,47.0 1305.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1060,184.0 L1052,172.0 1068,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-10\" stroke-width=\"2px\" d=\"M1150,182.0 C1150,92.0 1300.0,92.0 1300.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1150,184.0 L1142,172.0 1158,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-11\" stroke-width=\"2px\" d=\"M1240,182.0 C1240,137.0 1295.0,137.0 1295.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1240,184.0 L1232,172.0 1248,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-12\" stroke-width=\"2px\" d=\"M1420,182.0 C1420,137.0 1475.0,137.0 1475.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1420,184.0 L1412,172.0 1428,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f78e9946f01b47a8837dfedfc143fade-0-13\" stroke-width=\"2px\" d=\"M1330,182.0 C1330,92.0 1480.0,92.0 1480.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f78e9946f01b47a8837dfedfc143fade-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1480.0,184.0 L1488.0,172.0 1472.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Get HTML\n",
    "html = displacy.render(custom_ellipsis_doc, style=\"dep\", \n",
    "                       options={\"distance\": 90}, jupyter=False)\n",
    "\n",
    "# Display directly in Jupyter notebook\n",
    "display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a2dbc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guy', 'mind', 'forgotwhat', 'say', 'think']\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# Define a Boolean function to filter tokens\n",
    "def allowed_token(token):\n",
    "    return bool(\n",
    "        token\n",
    "        and str(token).strip()\n",
    "        and not token.is_stop\n",
    "        and not token.is_punct\n",
    "    )\n",
    "\n",
    "# Define a function to lemmatize, strip, and lowercase tokens\n",
    "def preprocess_token(token):\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "filtered_tokens = [\n",
    "    preprocess_token(token)\n",
    "    for token in custom_ellipsis_doc\n",
    "    if allowed_token(token)\n",
    "]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77579e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use rule-based matching to find full names in a text\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "def extract_full_names(nlp_doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}]\n",
    "    matcher.add(\"FULL_NAME\", [pattern]) # FULL_NAME is just a placeholder\n",
    "    matches = matcher(nlp_doc) # matches is a list of tuples\n",
    "    for _, start, end in matches:\n",
    "        yield nlp_doc[start:end].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4256a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earnings Call\n",
      "Call Participants\n",
      "Stewart Stecker\n",
      "Stewart Stecker\n",
      "Senior Director\n"
     ]
    }
   ],
   "source": [
    "# Print the first five full names found in the NVIDIA document\n",
    "for i, name in enumerate(extract_full_names(nvidia_doc), start=1):\n",
    "    if i > 5:\n",
    "        break\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6aebc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.14 (main, Oct 31 2025, 23:04:14) [Clang 21.1.4 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (learning_python)",
   "language": "python",
   "name": "learning_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
