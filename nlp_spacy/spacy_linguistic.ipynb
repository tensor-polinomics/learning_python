{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49cc9d90",
   "metadata": {},
   "source": [
    "# spaCy Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde6e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d8d53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.14 (main, Oct 31 2025, 23:04:14) [Clang 21.1.4 ]\n",
      "Executable: /mnt/ebs1/yluo/projects/learning/learning_python/.venv/bin/python\n",
      "Kernel: learning_python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Executable: {sys.executable}\")\n",
    "print(f\"Kernel: learning_python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c818e9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I                   ; Case=Nom|Number=Sing|Person=1|PronType=Prs; PRON\n",
      "was                 ; Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin; AUX\n",
      "reading             ; Aspect=Prog|Tense=Pres|VerbForm=Part; VERB\n",
      "the                 ; Definite=Def|PronType=Art; DET\n",
      "paper               ; Number=Sing; NOUN\n",
      "at                  ; ; ADP\n",
      "2PM                 ; NumType=Card; NUM\n",
      "that                ; Number=Sing|PronType=Dem; DET\n",
      "day                 ; Number=Sing; NOUN\n",
      ".                   ; PunctType=Peri; PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I was reading the paper at 2PM that day.\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:20}; {token.morph}; {token.pos_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a11783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "print(lemmatizer.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f3c93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.9'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e95c910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk: Autonomous cars, root text: cars, root dep: nsubj, root head text: shift\n",
      "chunk: insurance liability, root text: liability, root dep: dobj, root head text: shift\n",
      "chunk: manufacturers, root text: manufacturers, root dep: pobj, root head text: toward\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"Autonomous cars shift insurance liability toward manufacturers.\")\n",
    "for chunk in doc1.noun_chunks:\n",
    "    print(f\"chunk: {chunk.text}, root text: {chunk.root.text}, root dep: {chunk.root.dep_}, root head text: {chunk.root.head.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bbd7b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Autonomous; []\n",
      "Token: cars; [Autonomous]\n",
      "Token: shift; [cars, liability, toward, .]\n",
      "Token: insurance; []\n",
      "Token: liability; [insurance]\n",
      "Token: toward; [manufacturers]\n",
      "Token: manufacturers; []\n",
      "Token: .; []\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(f\"Token: {token.text}; {[child for child in token.children]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5741fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit nmod 0 2 ['holders', 'submit']\n",
      "and cc 0 0 ['Credit', 'holders', 'submit']\n",
      "mortgage compound 0 0 ['account', 'Credit', 'holders', 'submit']\n",
      "account conj 1 0 ['Credit', 'holders', 'submit']\n",
      "holders nsubj 1 0 ['submit']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Credit and mortgage account holders must submit their requests\")\n",
    "\n",
    "root = [token for token in doc if token.head == token][0]\n",
    "subject = list(root.lefts)[0]\n",
    "for descendant in subject.subtree:\n",
    "    assert subject is descendant or subject.is_ancestor(descendant)\n",
    "    print(descendant.text, descendant.dep_, descendant.n_lefts,\n",
    "            descendant.n_rights,\n",
    "            [ancestor.text for ancestor in descendant.ancestors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e430daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit and mortgage account holders NOUN nsubj submit\n",
      "must AUX aux submit\n",
      "submit VERB ROOT submit\n",
      "their PRON poss requests\n",
      "requests NOUN dobj submit\n"
     ]
    }
   ],
   "source": [
    "span = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(span)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74d41447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net income --> $9.4 million\n",
      "the prior year --> $2.7 million\n",
      "Revenue --> twelve billion dollars\n",
      "a loss --> 1b\n"
     ]
    }
   ],
   "source": [
    "# Merge noun phrases and entities for easier analysis\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n",
    "\n",
    "TEXTS = [\n",
    "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "]\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    for token in doc:\n",
    "        if token.ent_type_ == \"MONEY\":\n",
    "            # We have an attribute and direct object, so check for subject\n",
    "            if token.dep_ in (\"attr\", \"dobj\"):\n",
    "                subj = [w for w in token.head.lefts if w.dep_ == \"nsubj\"]\n",
    "                if subj:\n",
    "                    print(subj[0], \"-->\", token)\n",
    "            # We have a prepositional object with a preposition\n",
    "            elif token.dep_ == \"pobj\" and token.head.dep_ == \"prep\":\n",
    "                print(token.head.head, \"-->\", token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc6431c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('San Francisco', 0, 13, 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"San Francisco considers banning sidewalk delivery robots\")\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89e56e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['San Francisco', 'B', 'GPE']\n",
      "['considers', 'O', '']\n"
     ]
    }
   ],
   "source": [
    "# token level\n",
    "ent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]\n",
    "ent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]\n",
    "print(ent_san)  # ['San', 'B', 'GPE']\n",
    "print(ent_francisco)  # ['Francisco', 'I', 'GPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bacef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before []\n",
      "After [('fb', 0, 1, 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "\n",
    "doc = nlp(\"fb is hiring a new vice president of global policy\")\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('Before', ents)\n",
    "# The model didn't recognize \"fb\" as an entity :(\n",
    "\n",
    "# Create a span for the new entity\n",
    "fb_ent = Span(doc, 0, 1, label=\"ORG\")\n",
    "orig_ents = list(doc.ents)\n",
    "\n",
    "# Option 1: Modify the provided entity spans, leaving the rest unmodified\n",
    "doc.set_ents([fb_ent], default=\"unmodified\")\n",
    "\n",
    "# Option 2: Assign a complete list of ents to doc.ents\n",
    "doc.ents = orig_ents + [fb_ent]\n",
    "\n",
    "ents = [(e.text, e.start, e.end, e.label_) for e in doc.ents]\n",
    "print('After', ents)\n",
    "# [('fb', 0, 1, 'ORG')] ðŸŽ‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84321d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ()\n",
      "After (London,)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import spacy\n",
    "from spacy.attrs import ENT_IOB, ENT_TYPE\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp.make_doc(\"London is a big city in the United Kingdom.\")\n",
    "print(\"Before\", doc.ents)  # []\n",
    "\n",
    "header = [ENT_IOB, ENT_TYPE]\n",
    "attr_array = numpy.zeros((len(doc), len(header)), dtype=\"uint64\")\n",
    "attr_array[0, 0] = 3  # B\n",
    "attr_array[0, 1] = doc.vocab.strings[\"GPE\"]\n",
    "doc.from_array(header, attr_array)\n",
    "print(\"After\", doc.ents)  # [London]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6093d54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gimme', 'that']\n",
      "['gim', 'me', 'that']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"gimme that\")  # phrase to tokenize\n",
    "print([w.text for w in doc])  # ['gimme', 'that']\n",
    "\n",
    "# Add special case rule\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "\n",
    "# Check new tokenization\n",
    "print([w.text for w in nlp(\"gimme that\")])  # ['gim', 'me', 'that']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb9732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" \\t PREFIX\n",
      "Let \\t SPECIAL-1\n",
      "'s \\t SPECIAL-2\n",
      "go \\t TOKEN\n",
      "! \\t SUFFIX\n",
      "\" \\t SUFFIX\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "text = '''\"Let's go!\"'''\n",
    "doc = nlp(text)\n",
    "tok_exp = nlp.tokenizer.explain(text)\n",
    "assert [t.text for t in doc if not t.is_space] == [t[1] for t in tok_exp]\n",
    "for t in tok_exp:\n",
    "    print(t[1], \"\\\\t\", t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca82b7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What's\", 'happened', 'to', 'me?', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(\" \")\n",
    "        spaces = [True] * len(words)\n",
    "        # Avoid zero-length tokens\n",
    "        for i, word in enumerate(words):\n",
    "            if word == \"\":\n",
    "                words[i] = \" \"\n",
    "                spaces[i] = False\n",
    "        # Remove the final trailing space\n",
    "        if words[-1] == \" \":\n",
    "            words = words[0:-1]\n",
    "            spaces = spaces[0:-1]\n",
    "        else:\n",
    "           spaces[-1] = False\n",
    "\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
    "doc = nlp(\"What's happened to me? he thought. It wasn't a dream.\")\n",
    "print([token.text for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4001f56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]justin drew bi##eber is a canadian singer, songwriter, and actor.[SEP]  ['[CLS]', 'justin', 'drew', 'bi', '##eber', 'is', 'a', 'canadian', 'singer', ',', 'songwriter', ',', 'and', 'actor', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "\n",
    "class BertTokenizer:\n",
    "    def __init__(self, vocab, vocab_file, lowercase=True):\n",
    "        self.vocab = vocab\n",
    "        self._tokenizer = BertWordPieceTokenizer(vocab_file, lowercase=lowercase)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = self._tokenizer.encode(text)\n",
    "        words = []\n",
    "        spaces = []\n",
    "        for i, (text, (start, end)) in enumerate(zip(tokens.tokens, tokens.offsets)):\n",
    "            words.append(text)\n",
    "            if i < len(tokens.tokens) - 1:\n",
    "                # If next start != current end we assume a space in between\n",
    "                next_start, next_end = tokens.offsets[i + 1]\n",
    "                spaces.append(next_start > end)\n",
    "            else:\n",
    "                spaces.append(True)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.tokenizer = BertTokenizer(nlp.vocab, \"bert-base-uncased-vocab.txt\")\n",
    "doc = nlp(\"Justin Drew Bieber is a Canadian singer, songwriter, and actor.\")\n",
    "print(doc.text, [token.text for token in doc])\n",
    "# [CLS]justin drew bi##eber is a canadian singer, songwriter, and actor.[SEP]\n",
    "# ['[CLS]', 'justin', 'drew', 'bi', '##eber', 'is', 'a', 'canadian', 'singer',\n",
    "#  ',', 'songwriter', ',', 'and', 'actor', '.', '[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef7a74e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@spacy.registry.tokenizers(\"bert_word_piece_tokenizer\")\n",
    "def create_bert_tokenizer(vocab_file: str, lowercase: bool):\n",
    "    def create_tokenizer(nlp):\n",
    "        return BertTokenizer(nlp.vocab, vocab_file, lowercase)\n",
    "\n",
    "    return create_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab98d8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n",
      "[('Hello', 'Hello', ''), (',', ', ', ' '), ('world', 'world', ''), ('!', '!', '')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "words = [\"Hello\", \",\", \"world\", \"!\"]\n",
    "spaces = [False, True, False, False]\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "print([(t.text, t.text_with_ws, t.whitespace_) for t in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bae0fce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a -> b, lengths: [1 1 1 1 1 1 1 1]\n",
      "a -> b, mapping: [0 1 2 3 4 4 5 6]\n",
      "b -> a, lengths: [1 1 1 1 2 1 1]\n",
      "b -> a, mappings: [0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "from spacy.training import Alignment\n",
    "\n",
    "other_tokens = [\"i\", \"listened\", \"to\", \"obama\", \"'\", \"s\", \"podcasts\", \".\"]\n",
    "spacy_tokens = [\"i\", \"listened\", \"to\", \"obama\", \"'s\", \"podcasts\", \".\"]\n",
    "align = Alignment.from_strings(other_tokens, spacy_tokens)\n",
    "print(f\"a -> b, lengths: {align.x2y.lengths}\")  # array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "print(f\"a -> b, mapping: {align.x2y.data}\")  # array([0, 1, 2, 3, 4, 4, 5, 6]) : two tokens both refer to \"'s\"\n",
    "print(f\"b -> a, lengths: {align.y2x.lengths}\")  # array([1, 1, 1, 1, 2, 1, 1])   : the token \"'s\" refers to two tokens\n",
    "print(f\"b -> a, mappings: {align.y2x.data}\")  # array([0, 1, 2, 3, 4, 5, 6, 7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ef82706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['I', 'live', 'in', 'New', 'York']\n",
      "After: ['I', 'live', 'in', 'New York']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I live in New York\")\n",
    "print(\"Before:\", [token.text for token in doc])\n",
    "\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[3:5], attrs={\"LEMMA\": \"new york\"})\n",
    "print(\"After:\", [token.text for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1763eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "assert doc.has_annotation(\"SENT_START\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce7d289f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\"])\n",
    "nlp.enable_pipe(\"senter\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d3188eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()  # just the language with no pipeline\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49af08e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT PRON\n",
      "WP PRON\n",
      "NNP PROPN\n",
      "NNP PROPN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"I saw The Who perform. Who did you see?\"\n",
    "doc1 = nlp(text)\n",
    "print(doc1[2].tag_, doc1[2].pos_)  # DT DET\n",
    "print(doc1[3].tag_, doc1[3].pos_)  # WP PRON\n",
    "\n",
    "# Add attribute ruler with exception for \"The Who\" as NNP/PROPN NNP/PROPN\n",
    "ruler = nlp.get_pipe(\"attribute_ruler\")\n",
    "# Pattern to match \"The Who\"\n",
    "patterns = [[{\"LOWER\": \"the\"}, {\"TEXT\": \"Who\"}]]\n",
    "# The attributes to assign to the matched token\n",
    "attrs = {\"TAG\": \"NNP\", \"POS\": \"PROPN\"}\n",
    "# Add rules to the attribute ruler\n",
    "ruler.add(patterns=patterns, attrs=attrs, index=0)  # \"The\" in \"The Who\"\n",
    "ruler.add(patterns=patterns, attrs=attrs, index=1)  # \"Who\" in \"The Who\"\n",
    "\n",
    "doc2 = nlp(text)\n",
    "print(doc2[2].tag_, doc2[2].pos_)  # NNP PROPN\n",
    "print(doc2[3].tag_, doc2[3].pos_)  # NNP PROPN\n",
    "# The second \"Who\" remains unmodified\n",
    "print(doc2[5].tag_, doc2[5].pos_)  # WP PRON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f7caa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 7.443447 False\n",
      "cat True 7.443447 False\n",
      "banana True 6.895898 False\n",
      "afskfsd False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86146af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.8015960454940796\n",
      "salty fries <-> hamburgers 0.5733411312103271\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")  # make sure to use larger package!\n",
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864f37a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (learning_python)",
   "language": "python",
   "name": "learning_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
